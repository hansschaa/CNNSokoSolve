{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "noSolutionDF = pd.read_csv('output_nosol - copia 2.csv')\n",
    "solutionDF = pd.read_csv('output_sol - copia 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "[0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "noSolutionCount = noSolutionDF.shape[0]\n",
    "solutionCount = solutionDF.shape[0]\n",
    "\n",
    "print(noSolutionCount)\n",
    "print(solutionCount)\n",
    "\n",
    "noSolutionLabel = [0] * noSolutionCount \n",
    "solutionLabel = [1] * solutionCount \n",
    "Y = noSolutionLabel + solutionLabel\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tableros: 6\n"
     ]
    }
   ],
   "source": [
    "boards = []\n",
    "\n",
    "for indice, fila in noSolutionDF.iterrows():\n",
    "    tableros_en_celda = fila['Board'].replace(\"\\r\", \"\").split('\\n')\n",
    "    tableros_en_celda = list(filter(lambda x: x != '', tableros_en_celda))\n",
    "    tablero_matriz = [list(fila) for fila in tableros_en_celda]\n",
    "    boards.append(tablero_matriz)\n",
    "  \n",
    "    \n",
    "\n",
    "for indice, fila in solutionDF.iterrows():\n",
    "    tableros_en_celda = fila['Board'].replace(\"\\r\", \"\").split('\\n')\n",
    "    tableros_en_celda = list(filter(lambda x: x != '', tableros_en_celda))\n",
    "    tablero_matriz = [list(fila) for fila in tableros_en_celda]\n",
    "    boards.append(tablero_matriz)\n",
    "\n",
    "print(\"Total de tableros:\", len(boards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]]), array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]]), array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]]), array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]]), array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]]), array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]])]\n"
     ]
    }
   ],
   "source": [
    "def tablero_a_tensor(tablero):\n",
    "    alto = len(tablero)\n",
    "    ancho = len(tablero[0])\n",
    "    tensor = np.zeros((alto, ancho, 7))\n",
    "    for i in range(alto):\n",
    "        for j in range(ancho):\n",
    "            elemento = tablero[i][j]\n",
    "            if elemento == '#':\n",
    "                tensor[i, j, 0] = 1\n",
    "            elif elemento == '$':\n",
    "                tensor[i, j, 1] = 1\n",
    "            elif elemento == '.':\n",
    "                tensor[i, j, 2] = 1\n",
    "            elif elemento == '@':\n",
    "                tensor[i, j, 3] = 1\n",
    "            elif elemento == '*':\n",
    "                tensor[i, j, 4] = 1\n",
    "            elif elemento == '+':\n",
    "                tensor[i, j, 5] = 1\n",
    "            elif elemento == ' ':\n",
    "                tensor[i, j, 6] = 1\n",
    "                \n",
    "    return tensor\n",
    "\n",
    "X = [tablero_a_tensor(tablero) for tablero in boards]\n",
    "\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]]), array([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]])]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 0.8  \n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train = X[:split_index]\n",
    "X_val = X[split_index:]\n",
    "\n",
    "Y_train = Y[:split_index]\n",
    "Y_val = Y[split_index:]\n",
    "\n",
    "print(X_val)\n",
    "print(Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, _X, _Y, _name):\n",
    "        'Initialization'\n",
    "        self.batch_size = 1\n",
    "        self._X = _X\n",
    "        self._Y = _Y\n",
    "        self._name = _name\n",
    "        self.n_channels = 7\n",
    "        self.n_classes = 2\n",
    "        self.shuffle = False\n",
    "        self.on_init()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        print(f\"---------> [{self._name}]__getitem__\")\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        print(f\"- Indexes: {indexes}\")\n",
    "        #print(\"Index\")\n",
    "        #print(index)\n",
    "        # Find list of IDs\n",
    "        X = np.array([self._X[index]])\n",
    "        #list_IDs_Y = [self._Y[k] for k in indexes]\n",
    "        Y = self._Y[index]\n",
    "        #print(\"Values\")\n",
    "        #print(X)\n",
    "        \n",
    "        #print(list_IDs_X)\n",
    "        \n",
    "        #X, y = self.__data_generation(list_IDs_X, list_IDs_Y)\n",
    "        #print(np.array([X]))\n",
    "        #X = np.array([X])\n",
    "\n",
    "       # Hardcoded data for demonstration\n",
    "        '''X = np.array([[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
    "                       \n",
    "                       [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
    "                       \n",
    "                       [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
    "                       \n",
    "                       [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
    "                       \n",
    "                       [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]]])'''\n",
    "        \n",
    "        \n",
    "\n",
    "        y = np.array([0])  # Etiquetas de ejemplo\n",
    "        \n",
    "        #print(self.index)  \n",
    "        #print(y)\n",
    "        \n",
    "\n",
    "        #return X, keras.utils.to_categorical(0, num_classes=2)\n",
    "        return X, y\n",
    "    \n",
    "    def on_init(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        print(f\"---------> [{self._name}] Init\")\n",
    "        self.indexes = np.arange(len(self._X))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            print(self.indexes)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(f\"---------> [{self._name}]on_epoch_end\")\n",
    "        self.indexes = np.arange(len(self._X))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            print(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_X, list_IDs_Y):\n",
    "        'Generates data containing batch_size samples'\n",
    "        \n",
    "        #print(\"__data_generation\")\n",
    "        #ID = list_IDs_temp[0]\n",
    "        #print(\"ID\")\n",
    "        #print(ID)\n",
    "        X = list_IDs_X[0]\n",
    "        Y = list_IDs_Y[0]\n",
    "      \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------> [train] Init\n",
      "---------> [val] Init\n",
      "---------> [train]__getitem__\n",
      "- Indexes: [0]\n",
      "Epoch 1/4\n",
      "---------> [train]__getitem__\n",
      "- Indexes: [0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.6931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "---------> [train]on_epoch_end\n",
      "---------> [val]__getitem__\n",
      "- Indexes: [0]\n",
      "---------> [val]__getitem__\n",
      "- Indexes: [0]\n",
      "---------------------------------------------\n",
      "---------> [val]on_epoch_end\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.6931 - val_accuracy: 0.0000e+00 - val_loss: 0.6975\n",
      "Epoch 2/4\n",
      "---------> [train]__getitem__\n",
      "- Indexes: [0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.6926---------------------------------------------\n",
      "---------> [train]on_epoch_end\n",
      "---------> [val]__getitem__\n",
      "- Indexes: [0]\n",
      "---------------------------------------------\n",
      "---------> [val]on_epoch_end\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.6926 - val_accuracy: 0.0000e+00 - val_loss: 0.6970\n",
      "Epoch 3/4\n",
      "---------> [train]__getitem__\n",
      "- Indexes: [0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.6921---------------------------------------------\n",
      "---------> [train]on_epoch_end\n",
      "---------> [val]__getitem__\n",
      "- Indexes: [0]\n",
      "---------------------------------------------\n",
      "---------> [val]on_epoch_end\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6921 - val_accuracy: 0.0000e+00 - val_loss: 0.6965\n",
      "Epoch 4/4\n",
      "---------> [train]__getitem__\n",
      "- Indexes: [0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.6916---------------------------------------------\n",
      "---------> [train]on_epoch_end\n",
      "---------> [val]__getitem__\n",
      "- Indexes: [0]\n",
      "---------------------------------------------\n",
      "---------> [val]on_epoch_end\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6916 - val_accuracy: 0.0000e+00 - val_loss: 0.6960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"plt.figure(figsize=(12, 6))\\nplt.plot(history.history['accuracy'], label='Training Accuracy')\\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation, Conv2D, GlobalMaxPooling2D,GlobalAveragePooling2D, Dense, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=23)\n",
    "#print(X_train[0].shape, len(Y_train), len(X_test), len(Y_test))\n",
    "\n",
    "'''X_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\n",
    "Y_train = tf.convert_to_tensor(Y_train, dtype = tf.float32)\n",
    "Y_test = tf.convert_to_tensor(Y_test, dtype = tf.float32)'''\n",
    "\n",
    "\n",
    "#(height, width, channels, batch)\n",
    "input_shape = (5,4, 7)  \n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Generators\n",
    "train_generator = DataGenerator(X_train, Y_train, \"train\")\n",
    "val_generator = DataGenerator(X_val, Y_val, \"val\")\n",
    "\n",
    "# Arq\n",
    "#dropout\n",
    "#regla para saber cuantos filtros probar\n",
    "x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = Conv2D(filters=64, kernel_size=1, activation='relu', padding='same')(x)\n",
    "# conv2d que transforma todo a x * y * 64\n",
    "x = Conv2D(filters=1, kernel_size=1, activation='relu', padding='same')(x)\n",
    "#Condensa todos los valores de x * y * 1 a un solo valor \n",
    "x = GlobalMaxPooling2D()(x)\n",
    "outputs = Dense(1, activation = \"sigmoid\")(x) # softmax output\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "# Entrenar el modelo usando fit\n",
    "\n",
    "model.fit(train_generator,epochs=4, batch_size=1, validation_data= val_generator)\n",
    "\n",
    "# Graficar\n",
    "'''plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
